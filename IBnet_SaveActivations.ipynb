{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import regularizers\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import utils\n",
    "import loggingreporter \n",
    "\n",
    "cfg = {}\n",
    "cfg['SGD_BATCHSIZE']    = 128\n",
    "cfg['SGD_LEARNINGRATE'] = 0.001\n",
    "cfg['NUM_EPOCHS']       = 5000\n",
    "cfg['FULL_MI']          = True\n",
    "\n",
    "# cfg['ACTIVATION'] = 'tanh'\n",
    "cfg['ACTIVATION'] = 'relu'\n",
    "\n",
    "# How many hidden neurons to put into each of the layers\n",
    "cfg['LAYER_DIMS'] = [10,7,5,4,3] # original IB network\n",
    "ARCH_NAME =  '-'.join(map(str,cfg['LAYER_DIMS']))\n",
    "\n",
    "# No validation data is created. Only TRAIN and TEST\n",
    "trn, tst = utils.get_IB_data('2017_12_21_16_51_3_275766')\n",
    "\n",
    "# Where to save activation and weights data\n",
    "cfg['SAVE_DIR'] = 'rawdata/' + cfg['ACTIVATION'] + '_' + ARCH_NAME "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = keras.layers.Input((trn.X.shape[1],))\n",
    "clayer = input_layer\n",
    "# Truncated norm initalisation is recommended for neural network weights, but why devided by sqrt(layer dimmension)?\n",
    "for n in cfg['LAYER_DIMS']:\n",
    "    clayer = keras.layers.Dense(n, \n",
    "                                activation=cfg['ACTIVATION'],\n",
    "                                kernel_initializer=keras.initializers.TruncatedNormal(mean=0.0, stddev=1/np.sqrt(float(n)), seed=10),\n",
    "                                bias_initializer='zeros'\n",
    "#                                 kernel_regularizer=regularizers.l2(0.01) # add L2 norm regularization\n",
    "                               )(clayer)\n",
    "#     clayer = keras.layers.Dropout(0.1)(clayer)\n",
    "output_layer = keras.layers.Dense(trn.nb_classes, activation='softmax')(clayer)\n",
    "\n",
    "model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "# Tune optimizer & learning rate as well? perhaps too much..\n",
    "# optimizer = keras.optimizers.TFOptimizer(tf.train.AdamOptimizer(learning_rate=cfg['SGD_LEARNINGRATE']))\n",
    "\n",
    "# sgd = keras.optimizers.SGD(lr=cfg['SGD_LEARNINGRATE'], decay=1e-6, momentum=0.9, nesterov=True)\n",
    "# This is to study how optimizer with gradient clipping behaves in information plane\n",
    "sgd = keras.optimizers.SGD(lr=cfg['SGD_LEARNINGRATE'], momentum=0.9, nesterov=True, clipnorm=1.0, clipvalue=1)\n",
    "model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def do_report(epoch):\n",
    "    # Only log activity for some epochs.  Mainly this is to make things run faster.\n",
    "    if epoch < 20:       # Log for all first 20 epochs\n",
    "        return True\n",
    "    elif epoch < 100:    # Then for every 5th epoch\n",
    "        return (epoch % 5 == 0)\n",
    "    elif epoch < 2000:    # Then every 10th\n",
    "        return (epoch % 20 == 0)\n",
    "    else:                # Then every 100th\n",
    "        return (epoch % 100 == 0)\n",
    "    \n",
    "# This is the callback function used to save parameters used to calculate MI. Note that MI is not calculated here.\n",
    "reporter = loggingreporter.LoggingReporter(cfg=cfg, \n",
    "                                          trn=trn, \n",
    "                                          tst=tst, \n",
    "                                          do_save_func=do_report)\n",
    "r = model.fit(x=trn.X, y=trn.Y, \n",
    "              verbose    = 2, \n",
    "              batch_size = cfg['SGD_BATCHSIZE'],\n",
    "              epochs     = cfg['NUM_EPOCHS'],\n",
    "              validation_split = 0.2,\n",
    "              shuffle = True,\n",
    "              # validation_data=(tst.X, tst.Y),\n",
    "              callbacks  = [reporter,])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
